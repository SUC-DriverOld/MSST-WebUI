# This webUI file is for MSST-WebUI-Clouds

import gradio as gr
import subprocess
import os
import sys
import re
import time
import tempfile
import shutil
import json
import requests
import yaml
import librosa
import numpy as np
import pandas as pd
import platform
import warnings
import locale
import threading
import psutil
from datetime import datetime
from ml_collections import ConfigDict
from mir_eval.separation import bss_eval_sources
from pydub import AudioSegment
from rich.console import Console
from torch import cuda, backends
from multiprocessing import cpu_count
from .download_models import download_model


PACKAGE_VERSION = "1.5"
PRESETS = "data/preset_data.json"
BACKUP = "backup/"
MODELS = "data/model_map.json"
WEBUI_CONFIG = "data/webui_config.json"
VR_MODEL = "data/vr_model.json"
MODEL_FOLDER = "pretrain/"
CONFIG_TEMPLATE_FOLDER = "configs_template/"
VERSION_CONFIG = "data/version.json"
TEMP_PATH = "temp"
MODEL_TYPE = ['bs_roformer', 'mel_band_roformer', 'segm_models', 'htdemucs', 'mdx23c', 'swin_upernet', 'bandit']
FFMPEG = "ffmpeg"
PYTHON = sys.executable

language_dict = {
    "Auto": "Auto",
    "ç®€ä½“ä¸­æ–‡": "zh_CN",
    "ç¹é«”ä¸­æ–‡": "zh_TW",
    "English": "en_US",
    "æ—¥æœ¬èª": "ja_JP",
    "ğŸ˜Š": "emoji"
    }

warnings.filterwarnings("ignore")
stop_all_threads = False

def setup_webui():
    print(f"Music-Source-Separation-Training-Inference-Webui v{PACKAGE_VERSION} For-Clouds")
    print(i18n("[INFO] è®¾å¤‡ä¿¡æ¯ï¼š") + str(get_device()))


def i18n(key):
    try:
        config = load_configs(WEBUI_CONFIG)
        if config["settings"]["language"]== "Auto":
            language = locale.getdefaultlocale()[0]
        else: language = config['settings']['language']
    except Exception:
        language = locale.getdefaultlocale()[0]
    if language == "zh_CN":
        return key
    if not os.path.exists(path=f"tools/i18n/locale/{language}.json"):
        language = "en_US"
    with open(file=f"tools/i18n/locale/{language}.json", mode="r", encoding="utf-8") as f:
        language_list = json.load(f)
    return language_list.get(key, key)


def get_device():
    try:
        if cuda.is_available():
            gpus = []
            n_gpu = cuda.device_count()
            for i in range(n_gpu):
                gpus.append(f"{i}: {cuda.get_device_name(i)}")
            if len(gpus) == 1:
                return gpus[0]
            return gpus
        elif backends.mps.is_available():
            return i18n("ä½¿ç”¨MPS")
        else:
            return i18n("æ— å¯ç”¨çš„åŠ é€Ÿè®¾å¤‡, ä½¿ç”¨CPU")
    except Exception:
        return i18n("è®¾å¤‡æ£€æŸ¥å¤±è´¥")


def get_platform():
    os_name = platform.system()
    os_version = platform.version()
    machine = platform.machine()
    return f"System: {os_name}, Version: {os_version}, Machine: {machine}"


def load_configs(config_path):
    if config_path.endswith('.json'):
        with open(config_path, 'r', encoding="utf-8") as f:
            return json.load(f)
    elif config_path.endswith('.yaml') or config_path.endswith('.yml'):
        with open(config_path, 'r') as f:
            return ConfigDict(yaml.load(f, Loader=yaml.FullLoader))


def save_configs(config, config_path):
    if config_path.endswith('.json'):
        with open(config_path, 'w', encoding="utf-8") as f:
            json.dump(config, f, indent=4)
    elif config_path.endswith('.yaml') or config_path.endswith('.yml'):
        with open(config_path, 'w') as f:
            yaml.dump(config.to_dict(), f)


def print_command(command):
    print("\033[32m" + "Use command: " + command + "\033[0m")


def load_msst_model():
    config = load_configs(MODELS)
    model_list = []
    for keys in config.keys():
        for model in config[keys]:
            model_list.append(model["name"])
    return model_list


def get_msst_model(model_name):
    config = load_configs(MODELS)
    webui_config = load_configs(WEBUI_CONFIG)
    main_link = webui_config['settings']['download_link']
    if main_link == "Auto":
        language = locale.getdefaultlocale()[0]
        if language in ["zh_CN", "zh_TW", "zh_HK", "zh_SG"]:
            main_link = "hf-mirror.com"
        else: main_link = "huggingface.co"
    for keys in config.keys():
        for model in config[keys]:
            if model["name"] == model_name:
                model_type = model["model_type"]
                model_path = os.path.join(MODEL_FOLDER, keys, model_name)
                config_path = model["config_path"]
                download_link = model["link"]
                download_link = download_link.replace("huggingface.co", main_link)
                return model_path, config_path, model_type, download_link
    raise gr.Error(i18n("æ¨¡å‹ä¸å­˜åœ¨!"))


def load_vr_model():
    config = load_configs(VR_MODEL)
    return config.keys()


def load_vr_model_stem(model):
    config = load_configs(VR_MODEL)
    for keys in config.keys():
        if keys == model:
            primary_stem = config[keys]["primary_stem"]
            secondary_stem = config[keys]["secondary_stem"]
            vr_primary_stem_only = gr.Checkbox(label=f"{primary_stem} Only", value=False, interactive=True)
            vr_secondary_stem_only = gr.Checkbox(label=f"{secondary_stem} Only", value=False, interactive=True)
            return vr_primary_stem_only, vr_secondary_stem_only
    raise gr.Error(i18n("æ¨¡å‹ä¸å­˜åœ¨!"))


def load_presets_list():
    config = load_configs(PRESETS)
    if config == {}:
        return [i18n("æ— é¢„è®¾")]
    return list(config.keys())


def save_training_config(train_model_type, train_config_path, train_dataset_type, train_dataset_path, train_valid_path, train_num_workers, train_device_ids, train_seed, train_pin_memory, train_use_multistft_loss, train_use_mse_loss, train_use_l1_loss, train_results_path, train_accelerate):
    try:
        config = load_configs(WEBUI_CONFIG)
        config['training']['model_type'] = train_model_type
        config['training']['config_path'] = train_config_path
        config['training']['dataset_type'] = train_dataset_type
        config['training']['dataset_path'] = train_dataset_path
        config['training']['valid_path'] = train_valid_path
        config['training']['num_workers'] = train_num_workers
        config['training']['device_ids'] = train_device_ids
        config['training']['seed'] = train_seed
        config['training']['pin_memory'] = train_pin_memory
        config['training']['use_multistft_loss'] = train_use_multistft_loss
        config['training']['use_mse_loss'] = train_use_mse_loss
        config['training']['use_l1_loss'] = train_use_l1_loss
        config['training']['accelerate'] = train_accelerate
        config['training']['results_path'] = train_results_path
        save_configs(config, WEBUI_CONFIG)
        return i18n("é…ç½®ä¿å­˜æˆåŠŸ!")
    except Exception as e:
        print(e)
        return i18n("é…ç½®ä¿å­˜å¤±è´¥!")


def reset_webui_config():
    try:
        config = load_configs(WEBUI_CONFIG)
        for key in config['training_backup']:
            config['training'][key] = config['training_backup'][key]
        for key in config['inference_backup']:
            config['inference'][key] = config['inference_backup'][key]
        for key in config['tools_backup']:
            config['tools'][key] = config['tools_backup'][key]
        
        save_configs(config, WEBUI_CONFIG)
        return i18n("è®°å½•é‡ç½®æˆåŠŸ, è¯·é‡å¯WebUIåˆ·æ–°! ")
    except Exception as e:
        print(e)
        return i18n("è®°å½•é‡ç½®å¤±è´¥!")


def init_selected_vr_model():
    webui_config = load_configs(WEBUI_CONFIG)
    config = load_configs(VR_MODEL)
    model = webui_config['inference']['vr_select_model']
    if not model:
        vr_primary_stem_only = i18n("ä»…è¾“å‡ºä¸»éŸ³è½¨")
        vr_secondary_stem_only = i18n("ä»…è¾“å‡ºæ¬¡éŸ³è½¨")
        return vr_primary_stem_only, vr_secondary_stem_only
    for keys in config.keys():
        if keys == model:
            primary_stem = config[keys]["primary_stem"]
            secondary_stem = config[keys]["secondary_stem"]
            vr_primary_stem_only = f"{primary_stem} Only"
            vr_secondary_stem_only = f"{secondary_stem} Only"
            return vr_primary_stem_only, vr_secondary_stem_only
    vr_primary_stem_only = i18n("ä»…è¾“å‡ºä¸»éŸ³è½¨")
    vr_secondary_stem_only = i18n("ä»…è¾“å‡ºæ¬¡éŸ³è½¨")
    return vr_primary_stem_only, vr_secondary_stem_only


def update_train_start_check_point(path):
    if not os.path.isdir(path):
        raise gr.Error(i18n("è¯·å…ˆé€‰æ‹©æ¨¡å‹ä¿å­˜è·¯å¾„! "))
    ckpt_files = [f for f in os.listdir(path) if f.endswith(('.ckpt', '.pth', '.th'))]
    return gr.Dropdown(label=i18n("åˆå§‹æ¨¡å‹"), choices=ckpt_files if ckpt_files else ["None"])


def update_inference_settings(selected_model):
    _, config_path, _, _ = get_msst_model(selected_model)
    config = load_configs(config_path)
    if config.inference.get('batch_size'):
        batch_size = gr.Textbox(label="batch_size", value=str(
            config.inference.get('batch_size')), interactive=True)
    else:
        batch_size = gr.Textbox(label="batch_size", value=i18n("è¯¥æ¨¡å‹ä¸æ”¯æŒä¿®æ”¹æ­¤å€¼"), interactive=False)
    if config.inference.get('dim_t'):
        dim_t = gr.Textbox(label="dim_t", value=str(
            config.inference.get('dim_t')), interactive=True)
    else:
            dim_t = gr.Textbox(label="dim_t", value=i18n("è¯¥æ¨¡å‹ä¸æ”¯æŒä¿®æ”¹æ­¤å€¼"), interactive=False)
    if config.inference.get('num_overlap'):
        num_overlap = gr.Textbox(label="num_overlap", value=str(
            config.inference.get('num_overlap')), interactive=True)
    else:
        num_overlap = gr.Textbox(label="num_overlap", value=i18n("è¯¥æ¨¡å‹ä¸æ”¯æŒä¿®æ”¹æ­¤å€¼"), interactive=False)
    if config.inference.get('normalize'):
        normalize = gr.Checkbox(label="normalize", value=config.inference.get('normalize'), interactive=True)
    else:
        normalize = gr.Checkbox(label=i18n("normalize (è¯¥æ¨¡å‹ä¸æ”¯æŒä¿®æ”¹æ­¤å€¼) "), value=False, interactive=False)
    return batch_size, dim_t, num_overlap, normalize


def save_config(selected_model, batch_size, dim_t, num_overlap, normalize):
    _, config_path, _, _ = get_msst_model(selected_model)
    config = load_configs(config_path)
    if config.inference.get('batch_size'):
        config.inference['batch_size'] = int(batch_size) if batch_size.isdigit() else None
    if config.inference.get('dim_t'):
        config.inference['dim_t'] = int(dim_t) if dim_t.isdigit() else None
    if config.inference.get('num_overlap'):
        config.inference['num_overlap'] = int(num_overlap) if num_overlap.isdigit() else None
    if config.inference.get('normalize'):
        config.inference['normalize'] = normalize
    save_configs(config, config_path)
    return i18n("é…ç½®ä¿å­˜æˆåŠŸ!")


def reset_config(selected_model):
    _, original_config_path, _, _ = get_msst_model(selected_model)
    dir_path, file_name = os.path.split(original_config_path)
    backup_dir_path = dir_path.replace("configs", "configs_backup", 1)
    backup_config_path = os.path.join(backup_dir_path, file_name)
    if os.path.exists(backup_config_path):
        shutil.copy(backup_config_path, original_config_path)
        update_inference_settings(selected_model)
        return i18n("é…ç½®é‡ç½®æˆåŠŸ!")
    else:
        return i18n("å¤‡ä»½é…ç½®æ–‡ä»¶ä¸å­˜åœ¨!")


def run_command(command):
    global stop_all_threads
    stop_all_threads = False
    print_command(command)
    try:
        process = subprocess.Popen(command, shell=True)
        proc = psutil.Process(process.pid)
        while process.poll() is None:
            if stop_all_threads:
                for child in proc.children(recursive=True):
                    child.terminate()
                process.terminate()
                stop_all_threads = False
                return
        if process.returncode != 0:
            raise gr.Error(i18n("å‘ç”Ÿé”™è¯¯! è¯·å‰å¾€ç»ˆç«¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"))
    except Exception as e:
        print(e)
        raise gr.Error(i18n("å‘ç”Ÿé”™è¯¯! è¯·å‰å¾€ç»ˆç«¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"))


def stop_all_thread():
    global stop_all_threads
    for thread in threading.enumerate():
        if thread.name in ["msst_inference", "vr_inference", "msst_training", "msst_valid"]:
            stop_all_threads = True
            gr.Info(i18n("å·²åœæ­¢è¿›ç¨‹"))


def run_multi_inference(selected_model, input_folder, store_dir, extract_instrumental, gpu_id, force_cpu):
    if not bool(re.match(r'^(\d+)(?:\s(?!\1)\d+)*$', gpu_id)):
        raise gr.Error(i18n("GPU IDæ ¼å¼é”™è¯¯, è¯·é‡æ–°è¾“å…¥"))
    if selected_model == "":
        raise gr.Error(i18n("è¯·é€‰æ‹©æ¨¡å‹"))
    if input_folder == "":
        raise gr.Error(i18n("è¯·é€‰æ‹©è¾“å…¥ç›®å½•"))
    if not os.path.exists(store_dir):
        os.makedirs(store_dir)
    if download_model("msst", selected_model):
        run_inference(selected_model, input_folder, store_dir,extract_instrumental, gpu_id, force_cpu)
        return i18n("å¤„ç†å®Œæˆ! åˆ†ç¦»å®Œæˆçš„éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜åœ¨") + store_dir


def run_inference(selected_model, input_folder, store_dir, extract_instrumental, gpu_id, force_cpu, extra_store_dir=None):
    if extra_store_dir and not os.path.exists(extra_store_dir):
        os.makedirs(extra_store_dir)
    start_check_point, config_path, model_type, _ = get_msst_model(selected_model)
    gpu_ids = gpu_id if not force_cpu else "0"
    extract_instrumental_option = "--extract_instrumental" if extract_instrumental else ""
    force_cpu_option = "--force_cpu" if force_cpu else ""
    extra_store_dir = f"--extra_store_dir \"{extra_store_dir}\"" if extra_store_dir else ""
    command = f"{PYTHON} msst_inference.py --model_type {model_type} --config_path \"{config_path}\" --start_check_point \"{start_check_point}\" --input_folder \"{input_folder}\" --store_dir \"{store_dir}\" --device_ids {gpu_ids} {extract_instrumental_option} {force_cpu_option} {extra_store_dir}"
    msst_inference = threading.Thread(target=run_command, args=(command,), name="msst_inference")
    msst_inference.start()
    msst_inference.join()


def vr_inference_multi(vr_select_model, vr_window_size, vr_aggression, vr_output_format, vr_use_cpu, vr_primary_stem_only, vr_secondary_stem_only, vr_multiple_audio_input, vr_store_dir, vr_batch_size, vr_normalization, vr_post_process_threshold, vr_invert_spect, vr_enable_tta, vr_high_end_process, vr_enable_post_process, vr_debug_mode):
    if not os.path.isdir(vr_multiple_audio_input):
        return i18n("è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶å¤¹")
    if not vr_select_model:
        return i18n("è¯·é€‰æ‹©æ¨¡å‹")
    if not vr_store_dir:
        return i18n("è¯·é€‰æ‹©è¾“å‡ºç›®å½•")
    if not os.path.exists(vr_store_dir):
        os.makedirs(vr_store_dir)
    if download_model("uvr", vr_select_model):
        vr_inference(vr_select_model, vr_window_size, vr_aggression, vr_output_format, vr_use_cpu, vr_primary_stem_only, vr_secondary_stem_only, vr_multiple_audio_input, vr_store_dir, vr_batch_size, vr_normalization, vr_post_process_threshold, vr_invert_spect, vr_enable_tta, vr_high_end_process, vr_enable_post_process, vr_debug_mode)
        return i18n("å¤„ç†å®Œæˆ, ç»“æœå·²ä¿å­˜è‡³") + vr_store_dir


def vr_inference(vr_select_model, vr_window_size, vr_aggression, vr_output_format, vr_use_cpu, vr_primary_stem_only, vr_secondary_stem_only, vr_audio_input, vr_store_dir, vr_batch_size, vr_normalization, vr_post_process_threshold, vr_invert_spect, vr_enable_tta, vr_high_end_process, vr_enable_post_process, vr_debug_mode, save_another_stem=False, extra_output_dir=None):
    config = load_configs(WEBUI_CONFIG)
    model_file_dir = config['settings']['uvr_model_dir']
    model_mapping = load_configs(VR_MODEL)
    primary_stem = model_mapping[vr_select_model]["primary_stem"]
    secondary_stem = model_mapping[vr_select_model]["secondary_stem"]
    audio_file = vr_audio_input
    model_filename = vr_select_model
    output_format = vr_output_format
    output_dir = vr_store_dir
    invert_spect = "--invert_spect" if vr_invert_spect else ""
    normalization = vr_normalization
    debug_mode = "--debug" if vr_debug_mode else ""
    if vr_primary_stem_only:
        if vr_secondary_stem_only:
            single_stem = ""
        else:
            single_stem = f"--single_stem \"{primary_stem}\""
    else:
        if vr_secondary_stem_only:
            single_stem = f"--single_stem \"{secondary_stem}\""
        else:
            single_stem = ""
    sample_rate = 44100
    vr_batch_size = int(vr_batch_size)
    vr_aggression = int(vr_aggression)
    use_cpu = "--use_cpu" if vr_use_cpu else ""
    vr_enable_tta = "--vr_enable_tta" if vr_enable_tta else ""
    vr_high_end_process = "--vr_high_end_process" if vr_high_end_process else ""
    vr_enable_post_process = "--vr_enable_post_process" if vr_enable_post_process else ""
    save_another_stem = "--save_another_stem" if save_another_stem else ""
    extra_output_dir = f"--extra_output_dir \"{extra_output_dir}\"" if extra_output_dir else ""
    command = f"{PYTHON} uvr_inference.py \"{audio_file}\" {debug_mode} --model_filename \"{model_filename}\" --output_format {output_format} --output_dir \"{output_dir}\" --model_file_dir \"{model_file_dir}\" {invert_spect} --normalization {normalization} {single_stem} --sample_rate {sample_rate} {use_cpu} --vr_batch_size {vr_batch_size} --vr_window_size {vr_window_size} --vr_aggression {vr_aggression} {vr_enable_tta} {vr_high_end_process} {vr_enable_post_process} --vr_post_process_threshold {vr_post_process_threshold} {save_another_stem} {extra_output_dir}"
    vr_inference = threading.Thread(target=run_command, args=(command,), name="vr_inference")
    vr_inference.start()
    vr_inference.join()


def update_model_name(model_type):
    if model_type == "UVR_VR_Models":
        model_map = load_vr_model()
        return gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹"), choices=model_map, interactive=True)
    else:
        model_map = load_msst_model()
        return gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹"), choices=model_map, interactive=True)


def update_model_stem(model_type, model_name):
    if model_type == "UVR_VR_Models":
        config = load_configs(VR_MODEL)
        for keys in config.keys():
            if keys == model_name:
                primary_stem = config[keys]["primary_stem"]
                secondary_stem = config[keys]["secondary_stem"]
                return gr.Dropdown(label=i18n("è¾“å‡ºéŸ³è½¨"), choices=[primary_stem, secondary_stem], interactive=True)
    else:
        return gr.Dropdown(label=i18n("è¾“å‡ºéŸ³è½¨"), choices=["primary_stem"], value="primary_stem", interactive=False)


def add_to_flow_func(model_type, model_name, stem, secondary_output, df):
    if not model_type or not model_name:
        return df
    if model_type == "UVR_VR_Models" and not stem:
        return df
    if model_type == "UVR_VR_Models" and stem == "primary_stem":
        return df
    if not secondary_output or secondary_output == "":
        secondary_output = "False"
    new_data = pd.DataFrame({"model_type": [model_type], "model_name": [model_name], "stem": [stem], "secondary_output": [secondary_output]})
    if df["model_type"].iloc[0] == "" or df["model_name"].iloc[0] == "":
        return new_data
    updated_df = pd.concat([df, new_data], ignore_index=True)
    return updated_df

def save_flow_func(preset_name, df):
    preset_data = load_configs(PRESETS)
    if preset_name is None or preset_name == "":
        output_message = i18n("è¯·å¡«å†™é¢„è®¾åç§°")
        preset_name_delete = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(preset_data.keys()))
        preset_name_select = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(preset_data.keys()))
        return output_message, preset_name_delete, preset_name_select
    preset_dict = {f"Step_{index + 1}": row.dropna().to_dict() for index, row in df.iterrows()}
    preset_data[preset_name] = preset_dict
    save_configs(preset_data, PRESETS)
    output_message = i18n("é¢„è®¾") + preset_name + i18n("ä¿å­˜æˆåŠŸ")
    preset_name_delete = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(preset_data.keys()))
    preset_name_select = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(preset_data.keys()))
    return output_message, preset_name_delete, preset_name_select


def reset_flow_func():
    return gr.Dataframe(pd.DataFrame({"model_type": [""], "model_name": [""], "stem": [""]}), interactive=False, label=None)


def load_preset(preset_name):
    preset_data = load_configs(PRESETS)
    if preset_name in preset_data.keys():
        preset_flow = pd.DataFrame({"model_type": [""], "model_name": [""], "stem": [""], "secondary_output": [""]})
        preset_dict = preset_data[preset_name]
        for key in preset_dict.keys():
            try:
                secondary_output = preset_dict[key]["secondary_output"]
            except KeyError:
                secondary_output = "False"
            preset_flow = add_to_flow_func(preset_dict[key]["model_type"], preset_dict[key]["model_name"], preset_dict[key]["stem"], secondary_output, preset_flow)
        return preset_flow
    return gr.Dataframe(pd.DataFrame({"model_type": [i18n("é¢„è®¾ä¸å­˜åœ¨")], "model_name": [i18n("é¢„è®¾ä¸å­˜åœ¨")], "stem": [i18n("é¢„è®¾ä¸å­˜åœ¨")], "secondary_output": [i18n("é¢„è®¾ä¸å­˜åœ¨")]}), interactive=False, label=None)


def delete_func(preset_name):
    preset_data = load_configs(PRESETS)
    if preset_name in preset_data.keys():
        _, select_preset_backup = backup_preset_func()
        del preset_data[preset_name]
        save_configs(preset_data, PRESETS)
        output_message = i18n("é¢„è®¾") + preset_name + i18n("åˆ é™¤æˆåŠŸ")
        preset_name_delete = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(preset_data.keys()))
        preset_name_select = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(preset_data.keys()))
        preset_flow_delete = gr.Dataframe(pd.DataFrame({"model_type": [i18n("é¢„è®¾å·²åˆ é™¤")], "model_name": [i18n("é¢„è®¾å·²åˆ é™¤")], "stem": [i18n("é¢„è®¾å·²åˆ é™¤")], "secondary_output": [i18n("é¢„è®¾å·²åˆ é™¤")]}), interactive=False, label=None)
        return output_message, preset_name_delete, preset_name_select, preset_flow_delete, select_preset_backup
    else:
        return i18n("é¢„è®¾ä¸å­˜åœ¨")


def run_inference_flow(input_folder, store_dir, preset_name, force_cpu):
    start_time = time.time()
    preset_data = load_configs(PRESETS)
    if not preset_name in preset_data.keys():
        return i18n("é¢„è®¾") + preset_name + i18n("ä¸å­˜åœ¨")
    config = load_configs(WEBUI_CONFIG)
    model_list = preset_data[preset_name]
    input_to_use = input_folder
    tmp_store_dir = tempfile.mkdtemp()
    i = 0
    for step in model_list.keys():
        if i == 0:
            input_to_use = input_folder
        elif i < len(model_list.keys()) - 1 and i > 0:
            if input_to_use != input_folder:
                shutil.rmtree(input_to_use)
            input_to_use = tmp_store_dir
            tmp_store_dir = tempfile.mkdtemp()
        elif i == len(model_list.keys()) - 1:
            input_to_use = tmp_store_dir
            tmp_store_dir = store_dir
        console = Console()
        model_name = model_list[step]["model_name"]
        console.rule(f"[yellow]Step {i+1}: Running inference using {model_name}", style="yellow")
        if model_list[step]["model_type"] == "MSST_Models":
            gpu_id = config['inference']['gpu_id'] if not force_cpu else "0"
            try:
                secondary_output = model_list[step]["secondary_output"]
            except KeyError:
                secondary_output = "False"
            if secondary_output == "True":
                extract_instrumental = True
                extra_store_dir = os.path.join(store_dir, "secondary_output")
            else:
                extract_instrumental = False
                extra_store_dir = None
            if download_model("msst", model_name):
                run_inference(model_name, input_to_use, tmp_store_dir, extract_instrumental, gpu_id, force_cpu, extra_store_dir)
        elif model_list[step]["model_type"] == "UVR_VR_Models":
            vr_model_config = load_configs(VR_MODEL)
            stem = model_list[step]["stem"]
            vr_select_model = model_name
            vr_window_size = config['inference']['vr_window_size']
            vr_aggression = config['inference']['vr_aggression']
            vr_output_format = "wav"
            vr_use_cpu = force_cpu
            vr_primary_stem_only = True if stem == vr_model_config[model_name]["primary_stem"] else False
            vr_secondary_stem_only = True if stem == vr_model_config[model_name]["secondary_stem"] else False
            vr_audio_input = input_to_use
            vr_store_dir = tmp_store_dir
            vr_batch_size = config['inference']['vr_batch_size']
            vr_normalization = config['inference']['vr_normalization']
            vr_post_process_threshold = config['inference']['vr_post_process_threshold']
            vr_invert_spect = config['inference']['vr_invert_spect']
            vr_enable_tta = config['inference']['vr_enable_tta']
            vr_high_end_process = config['inference']['vr_high_end_process']
            vr_enable_post_process = config['inference']['vr_enable_post_process']
            vr_debug_mode = config['inference']['vr_debug_mode']
            try:
                secondary_output = model_list[step]["secondary_output"]
            except KeyError:
                secondary_output = "False"
            if secondary_output == "True":
                save_another_stem = True
                extra_output_dir = os.path.join(store_dir, "secondary_output")
            else:
                save_another_stem = False
                extra_output_dir = None
            if download_model("uvr", vr_select_model):
                vr_inference(vr_select_model, vr_window_size, vr_aggression, vr_output_format, vr_use_cpu, vr_primary_stem_only, vr_secondary_stem_only, vr_audio_input, vr_store_dir, vr_batch_size, vr_normalization, vr_post_process_threshold, vr_invert_spect, vr_enable_tta, vr_high_end_process, vr_enable_post_process, vr_debug_mode, save_another_stem, extra_output_dir)
        i += 1
    if tmp_store_dir != store_dir:
        for file_name in os.listdir(tmp_store_dir):
            shutil.move(os.path.join(tmp_store_dir, file_name),
                        os.path.join(store_dir, file_name))
        shutil.rmtree(tmp_store_dir)
    finish_time = time.time()
    elapsed_time = finish_time - start_time
    Console().rule(f"[yellow]Finished runing {preset_name}! Costs {elapsed_time:.2f}s", style="yellow")
    return i18n("å¤„ç†å®Œæˆ! åˆ†ç¦»å®Œæˆçš„éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜åœ¨") + store_dir


def preset_backup_list():
    backup_dir = BACKUP
    if not os.path.exists(backup_dir):
        return [i18n("æš‚æ— å¤‡ä»½æ–‡ä»¶")]
    backup_files = []
    for file in os.listdir(backup_dir):
        if file.startswith("preset_backup_") and file.endswith(".json"):
            backup_files.append(file)
    if backup_files == []:
        return [i18n("æš‚æ— å¤‡ä»½æ–‡ä»¶")]
    return backup_files


def restore_preset_func(backup_file):
    backup_dir = BACKUP
    if not backup_file or backup_file == i18n("æš‚æ— å¤‡ä»½æ–‡ä»¶"):
        return i18n("è¯·é€‰æ‹©å¤‡ä»½æ–‡ä»¶")
    backup_data = load_configs(os.path.join(backup_dir, backup_file))
    save_configs(backup_data, PRESETS)
    output_message_manage = i18n("å·²æˆåŠŸæ¢å¤å¤‡ä»½") + backup_file
    preset_dropdown = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(backup_data.keys()))
    preset_name_delet = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=list(backup_data.keys()))
    preset_flow_delete = pd.DataFrame({"model_type": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")], "model_name": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")], "stem": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")], "secondary_output": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")]})
    return output_message_manage, preset_dropdown, preset_name_delet, preset_flow_delete


def backup_preset_func():
    backup_dir = BACKUP
    if not os.path.exists(backup_dir):
        os.makedirs(backup_dir)
    backup_file = f"preset_backup_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json"
    preset_data = load_configs(PRESETS)
    save_configs(preset_data, os.path.join(backup_dir, backup_file))
    msg = i18n("å·²æˆåŠŸå¤‡ä»½è‡³") + backup_file
    select_preset_backup = gr.Dropdown(label=i18n("é€‰æ‹©éœ€è¦æ¢å¤çš„é¢„è®¾æµç¨‹å¤‡ä»½"), choices=preset_backup_list(), interactive=True, scale=4)
    return msg, select_preset_backup


def convert_audio(uploaded_files, ffmpeg_output_format, ffmpeg_output_folder):
    if not uploaded_files:
        return i18n("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡ä»¶")
    success_files = []
    for uploaded_file in uploaded_files:
        uploaded_file_path = uploaded_file.name
        output_path = ffmpeg_output_folder
        if not os.path.exists(output_path):
            os.makedirs(output_path)
        output_file = os.path.join(output_path, os.path.splitext(
            os.path.basename(uploaded_file_path))[0] + "." + ffmpeg_output_format)
        command = f"{FFMPEG} -i \"{uploaded_file_path}\" \"{output_file}\""
        print_command(command)
        try:
            subprocess.run(command, shell=True, check=True)
            success_files.append(output_file)
        except subprocess.CalledProcessError:
            print(f"Fail to convert file: {uploaded_file_path}\n")
            continue
    if not success_files:
        return i18n("æ‰€æœ‰æ–‡ä»¶è½¬æ¢å¤±è´¥, è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œffmpegè·¯å¾„ã€‚")
    else:
        text = i18n("å¤„ç†å®Œæˆ, æ–‡ä»¶å·²ä¿å­˜ä¸º: ") + "\n" + "\n".join(success_files)
        return text


def merge_audios(input_folder, output_folder):
    combined_audio = AudioSegment.empty()
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    output_file = os.path.join(output_folder, "merged_audio.wav")
    for filename in sorted(os.listdir(input_folder)):
        if filename.endswith(('.mp3', '.wav', '.ogg', '.flac')):
            file_path = os.path.join(input_folder, filename)
            audio = AudioSegment.from_file(file_path)
            combined_audio += audio
    try:
        combined_audio.export(output_file, format="wav")
        return i18n("å¤„ç†å®Œæˆ, æ–‡ä»¶å·²ä¿å­˜ä¸º: ") + output_file
    except Exception as e:
        print(e)
        return i18n("å¤„ç†å¤±è´¥!")


def process_audio(true_path, estimated_path):
    true_audio, _ = librosa.load(true_path, sr=44100, mono=False)
    if true_audio.ndim == 1:
        true_audio = np.vstack((true_audio, true_audio))
    elif true_audio.ndim == 2 and true_audio.shape[0] == 1:
        true_audio = np.vstack((true_audio[0], true_audio[0]))
    estimated_audio, _ = librosa.load(estimated_path, sr=44100, mono=False)
    if estimated_audio.ndim == 1:
        estimated_audio = np.vstack((estimated_audio, estimated_audio))
    elif estimated_audio.ndim == 2 and estimated_audio.shape[0] == 1:
        estimated_audio = np.vstack((estimated_audio[0], estimated_audio[0]))
    min_length = min(true_audio.shape[1], estimated_audio.shape[1])
    true_audio = true_audio[:, :min_length]
    estimated_audio = estimated_audio[:, :min_length]
    sdr, _, _, _ = bss_eval_sources(true_audio, estimated_audio)
    print(f"SDR: {sdr}")
    return sdr


def ensemble(files, ensemble_mode, weights, output_path):
    if len(files) < 2:
        return i18n("è¯·ä¸Šä¼ è‡³å°‘2ä¸ªæ–‡ä»¶")
    if len(files) != len(weights.split()):
        return i18n("ä¸Šä¼ çš„æ–‡ä»¶æ•°ç›®ä¸æƒé‡æ•°ç›®ä¸åŒ¹é…")
    else:
        files_argument = " ".join(files)
        if not os.path.exists(output_path):
            os.makedirs(output_path)
        output_path = os.path.join(output_path, f"ensemble_{ensemble_mode}.wav")
        command = f"{PYTHON} ensemble.py --files {files_argument} --type {ensemble_mode} --weights {weights} --output {output_path}"
        print_command(command)
        try:
            subprocess.run(command, shell = True)
            return i18n("å¤„ç†å®Œæˆ, æ–‡ä»¶å·²ä¿å­˜ä¸º: ") + output_path
        except Exception as e:
            return i18n("å¤„ç†å¤±è´¥!")


def some_inference(audio_file, bpm, output_dir):
    model = "tools/SOME_weights/model_steps_64000_simplified.ckpt"
    if not os.path.isfile(model):
        return i18n("è¯·å…ˆä¸‹è½½SOMEé¢„å¤„ç†æ¨¡å‹å¹¶æ”¾ç½®åœ¨tools/SOME_weightsæ–‡ä»¶å¤¹ä¸‹! ")
    if not audio_file.endswith('.wav'):
        return i18n("è¯·ä¸Šä¼ wavæ ¼å¼æ–‡ä»¶")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    tempo = int(bpm)
    file_name = os.path.basename(audio_file)[0:-4]
    midi = os.path.join(output_dir, f"{file_name}.mid")
    command = f"{PYTHON} tools/SOME/infer.py --model {model} --wav \"{audio_file}\" --midi \"{midi}\" --tempo {tempo}"
    print_command(command)
    try:
        subprocess.run(command, shell=True)
        return i18n("å¤„ç†å®Œæˆ, æ–‡ä»¶å·²ä¿å­˜ä¸º: ") + midi
    except Exception as e:
        return i18n("å¤„ç†å¤±è´¥!")


def start_training(train_model_type, train_config_path, train_dataset_type, train_dataset_path, train_valid_path, train_num_workers, train_device_ids, train_seed, train_pin_memory, train_use_multistft_loss, train_use_mse_loss, train_use_l1_loss, train_results_path, train_start_check_point, train_accelerate):
    model_type = train_model_type
    config_path = train_config_path
    start_check_point = train_start_check_point
    results_path = train_results_path
    data_path = train_dataset_path
    dataset_type = train_dataset_type
    valid_path = train_valid_path
    num_workers = int(train_num_workers)
    device_ids = train_device_ids
    seed = int(train_seed)
    pin_memory = train_pin_memory
    use_multistft_loss = "--use_multistft_loss" if train_use_multistft_loss else ""
    use_mse_loss = "--use_mse_loss" if train_use_mse_loss else ""
    use_l1_loss = "--use_l1_loss" if train_use_l1_loss else ""
    if train_accelerate:
        train_file = "train_accelerate.py"
    else:
        train_file = "train.py"
    if model_type not in MODEL_TYPE:
        return i18n("æ¨¡å‹ç±»å‹é”™è¯¯, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.isfile(config_path):
        return i18n("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.exists(results_path):
        os.makedirs(results_path)
    if not os.path.exists(data_path):
        return i18n("æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.exists(valid_path):
        return i18n("éªŒè¯é›†è·¯å¾„ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    if dataset_type not in [1, 2, 3, 4]:
        return i18n("æ•°æ®é›†ç±»å‹é”™è¯¯, è¯·é‡æ–°é€‰æ‹©")
    if not bool(re.match(r'^(\d+)(?:\s(?!\1)\d+)*$', device_ids)):
        return i18n("device_idsæ ¼å¼é”™è¯¯, è¯·é‡æ–°è¾“å…¥")
    if train_start_check_point == "None" or train_start_check_point == "":
        start_check_point = ""
    elif os.path.exists(results_path):
        start_check_point = "--start_check_point " + "\"" + os.path.join(results_path, train_start_check_point) + "\""
    else:
        return i18n("æ¨¡å‹ä¿å­˜è·¯å¾„ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    command = f"{PYTHON} {train_file} --model_type {model_type} --config_path \"{config_path}\" {start_check_point} --results_path \"{results_path}\" --data_path \"{data_path}\" --dataset_type {dataset_type} --valid_path \"{valid_path}\" --num_workers {num_workers} --device_ids {device_ids} --seed {seed} --pin_memory {pin_memory} {use_multistft_loss} {use_mse_loss} {use_l1_loss}"
    threading.Thread(target=run_command, args=(command,), name="msst_training").start()
    return i18n("è®­ç»ƒå¯åŠ¨æˆåŠŸ! è¯·å‰å¾€æ§åˆ¶å°æŸ¥çœ‹è®­ç»ƒä¿¡æ¯! ")


def validate_model(valid_model_type, valid_config_path, valid_model_path, valid_path, valid_results_path, valid_device_ids, valid_num_workers, valid_extension, valid_pin_memory):
    if valid_model_type not in MODEL_TYPE:
        return i18n("æ¨¡å‹ç±»å‹é”™è¯¯, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.isfile(valid_config_path):
        return i18n("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.isfile(valid_model_path):
        return i18n("æ¨¡å‹ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.exists(valid_path):
        return i18n("éªŒè¯é›†è·¯å¾„ä¸å­˜åœ¨, è¯·é‡æ–°é€‰æ‹©")
    if not os.path.exists(valid_results_path):
        os.makedirs(valid_results_path)
    if not bool(re.match(r'^(\d+)(?:\s(?!\1)\d+)*$', valid_device_ids)):
        return i18n("device_idsæ ¼å¼é”™è¯¯, è¯·é‡æ–°è¾“å…¥")
    pin_memory = "--pin_memory" if valid_pin_memory else ""
    command = f"{PYTHON} valid.py --model_type {valid_model_type} --config_path \"{valid_config_path}\" --start_check_point \"{valid_model_path}\" --valid_path \"{valid_path}\" --store_dir \"{valid_results_path}\" --device_ids {valid_device_ids} --num_workers {valid_num_workers} --extension {valid_extension} {pin_memory}"
    msst_valid = threading.Thread(target=run_command, args=(command,), name="msst_valid")
    msst_valid.start()
    msst_valid.join()
    return i18n("éªŒè¯å®Œæˆ! è¯·æ‰“å¼€è¾“å‡ºæ–‡ä»¶å¤¹æŸ¥çœ‹è¯¦ç»†ç»“æœ")


def check_webui_update():
    url = "https://github.com/SUC-DriverOld/MSST-WebUI/releases/latest"
    try:
        response = requests.get(url)
        response.raise_for_status()
        latest_version = response.url.split("/")[-1]
        if latest_version != PACKAGE_VERSION:
            return i18n("å½“å‰ç‰ˆæœ¬: ") + PACKAGE_VERSION + i18n(", å‘ç°æ–°ç‰ˆæœ¬: ") + latest_version
        else:
            return i18n("å½“å‰ç‰ˆæœ¬: ") + PACKAGE_VERSION + i18n(", å·²æ˜¯æœ€æ–°ç‰ˆæœ¬")
    except Exception:
        return i18n("æ£€æŸ¥æ›´æ–°å¤±è´¥")


def change_language(language):
    config = load_configs(WEBUI_CONFIG)
    if language in language_dict.keys():
        config['settings']['language'] = language_dict[language]
    else:
        config['settings']['language'] = "Auto"
    save_configs(config, WEBUI_CONFIG)
    return i18n("è¯­è¨€å·²æ›´æ”¹, é‡å¯WebUIç”Ÿæ•ˆ")


def get_language():
    config = load_configs(WEBUI_CONFIG)
    language = config['settings']['language']
    for key, value in language_dict.items():
        if value == language:
            return key
    return "Auto"


def change_download_link(link):
    config = load_configs(WEBUI_CONFIG)
    if link == i18n("huggingface.co (éœ€è¦é­”æ³•)"):
        config['settings']['download_link'] = "huggingface.co"
    elif link == i18n("hf-mirror.com (é•œåƒç«™å¯ç›´è¿)"):
        config['settings']['download_link'] = "hf-mirror.com"
    else:
        config['settings']['download_link'] = "Auto"
    save_configs(config, WEBUI_CONFIG)
    return i18n("ä¸‹è½½é“¾æ¥å·²æ›´æ”¹")


setup_webui()
with gr.Blocks(
        theme=gr.Theme.load('tools/themes/theme_schema@1.2.2.json')
) as app:
    gr.Markdown(value=f"""### Music-Source-Separation-Training-Inference-Webui v{PACKAGE_VERSION} For-Clouds""")
    gr.Markdown(value=i18n("ä»…ä¾›ä¸ªäººå¨±ä¹å’Œéå•†ä¸šç”¨é€”, ç¦æ­¢ç”¨äºè¡€è…¥/æš´åŠ›/æ€§ç›¸å…³/æ”¿æ²»ç›¸å…³å†…å®¹ã€‚[ç‚¹å‡»å‰å¾€æ•™ç¨‹æ–‡æ¡£](https://r1kc63iz15l.feishu.cn/wiki/JSp3wk7zuinvIXkIqSUcCXY1nKc)<br>æœ¬æ•´åˆåŒ…å®Œå…¨å…è´¹, ä¸¥ç¦ä»¥ä»»ä½•å½¢å¼å€’å–, å¦‚æœä½ ä»ä»»ä½•åœ°æ–¹**ä»˜è´¹**è´­ä¹°äº†æœ¬æ•´åˆåŒ…, è¯·**ç«‹å³é€€æ¬¾**ã€‚<br> æ•´åˆåŒ…ä½œè€…: [bilibili@é˜¿ç‹¸ä¸åƒéš¼èˆ](https://space.bilibili.com/403335715) [Github@KitsuneX07](https://github.com/KitsuneX07) | [Bilibili@Sucial](https://space.bilibili.com/445022409) [Github@SUC-DriverOld](https://github.com/SUC-DriverOld) | Gradioä¸»é¢˜: [Gradio Theme](https://huggingface.co/spaces/NoCrypt/miku)"))
    with gr.Tabs():
        webui_config = load_configs(WEBUI_CONFIG)
        presets = load_configs(PRESETS)
        models = load_configs(MODELS)
        vr_model = load_configs(VR_MODEL)

        with gr.TabItem(label=i18n("MSSTåˆ†ç¦»")):
            gr.Markdown(value=i18n("MSSTéŸ³é¢‘åˆ†ç¦»åŸé¡¹ç›®åœ°å€: [https://github.com/ZFTurbo/Music-Source-Separation-Training](https://github.com/ZFTurbo/Music-Source-Separation-Training)"))
            selected_model = gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹"), choices=load_msst_model(), value=None, interactive=True)
            with gr.Row():
                gpu_id = gr.Textbox(label=i18n("é€‰æ‹©ä½¿ç”¨çš„GPU ID, å¤šå¡ç”¨æˆ·è¯·ä½¿ç”¨ç©ºæ ¼åˆ†éš”GPU IDã€‚å¯å‰å¾€è®¾ç½®é¡µé¢æŸ¥çœ‹æ˜¾å¡ä¿¡æ¯ã€‚"), value="0",interactive=True)
                with gr.Column():
                    force_cpu = gr.Checkbox(label=i18n("ä½¿ç”¨CPU (æ³¨æ„: ä½¿ç”¨CPUä¼šå¯¼è‡´é€Ÿåº¦éå¸¸æ…¢) "), value=False, interactive=True)
                    extract_instrumental = gr.Checkbox(label=i18n("åŒæ—¶è¾“å‡ºæ¬¡çº§éŸ³è½¨"), value=False, interactive=True)
            multiple_audio_input = gr.Textbox(label=i18n("è¾“å…¥ç›®å½•"), value="input/", interactive=True)
            store_dir = gr.Textbox(label=i18n("è¾“å‡ºç›®å½•"), value="results/", interactive=True)
            with gr.Accordion(i18n("æ¨ç†å‚æ•°è®¾ç½® (ä¸€èˆ¬ä¸éœ€è¦åŠ¨) "), open=False):
                gr.Markdown(value=i18n("åªæœ‰åœ¨ç‚¹å‡»ä¿å­˜åæ‰ä¼šç”Ÿæ•ˆã€‚å‚æ•°ç›´æ¥å†™å…¥é…ç½®æ–‡ä»¶, æ— æ³•æ’¤é”€ã€‚å‡å¦‚ä¸çŸ¥é“å¦‚ä½•è®¾ç½®, è¯·ä¿æŒé»˜è®¤å€¼ã€‚<br>è¯·ç‰¢è®°è‡ªå·±ä¿®æ”¹å‰çš„å‚æ•°æ•°å€¼, é˜²æ­¢å‡ºç°é—®é¢˜ä»¥åæ— æ³•æ¢å¤ã€‚è¯·ç¡®ä¿è¾“å…¥æ­£ç¡®çš„å‚æ•°, å¦åˆ™å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ— æ³•æ­£å¸¸è¿è¡Œã€‚<br>å‡å¦‚ä¿®æ”¹åæ— æ³•æ¢å¤, è¯·ç‚¹å‡»``é‡ç½®``æŒ‰é’®, è¿™ä¼šä½¿å¾—é…ç½®æ–‡ä»¶æ¢å¤åˆ°é»˜è®¤å€¼ã€‚"))
                with gr.Row():
                    batch_size = gr.Textbox(label=i18n("batch_size: æ‰¹æ¬¡å¤§å°, ä¸€èˆ¬ä¸éœ€è¦æ”¹"), value=i18n("è¯·å…ˆé€‰æ‹©æ¨¡å‹"))
                    dim_t = gr.Textbox(label=i18n("dim_t: æ—¶åºç»´åº¦å¤§å°, ä¸€èˆ¬ä¸éœ€è¦æ”¹ (éƒ¨åˆ†æ¨¡å‹æ²¡æœ‰æ­¤å‚æ•°)"), value=i18n("è¯·å…ˆé€‰æ‹©æ¨¡å‹"))
                    num_overlap = gr.Textbox(label=i18n("num_overlap: çª—å£é‡å é•¿åº¦, æ•°å€¼è¶Šå°é€Ÿåº¦è¶Šå¿«, ä½†ä¼šç‰ºç‰²æ•ˆæœ"), value=i18n("è¯·å…ˆé€‰æ‹©æ¨¡å‹"))
                normalize = gr.Checkbox(label=i18n("normalize: æ˜¯å¦å¯¹éŸ³é¢‘è¿›è¡Œå½’ä¸€åŒ–å¤„ç† (éƒ¨åˆ†æ¨¡å‹æ²¡æœ‰æ­¤å‚æ•°)"), value=False, interactive=False)
                reset_config_button = gr.Button(i18n("é‡ç½®é…ç½®"), variant="secondary")
                save_config_button = gr.Button(i18n("ä¿å­˜é…ç½®"), variant="primary")
            inference_multiple = gr.Button(i18n("æ‰¹é‡éŸ³é¢‘åˆ†ç¦»"), variant="primary")
            with gr.Row():
                output_message = gr.Textbox(label="Output Message", scale=4)
                stop_thread = gr.Button(i18n("å¼ºåˆ¶åœæ­¢"), scale=1)

            inference_multiple.click(fn=run_multi_inference, inputs=[selected_model, multiple_audio_input, store_dir, extract_instrumental, gpu_id, force_cpu],outputs=output_message)
            selected_model.change(fn=update_inference_settings,inputs=[selected_model],outputs=[batch_size, dim_t, num_overlap, normalize])
            save_config_button.click(fn=save_config,inputs=[selected_model, batch_size, dim_t, num_overlap, normalize],outputs=output_message)
            reset_config_button.click(fn=reset_config,inputs=[selected_model],outputs=output_message)
            stop_thread.click(fn=stop_all_thread)

        with gr.TabItem(label=i18n("UVRåˆ†ç¦»")):
            gr.Markdown(value=i18n("è¯´æ˜: æœ¬æ•´åˆåŒ…ä»…èåˆäº†UVRçš„VR Architectureæ¨¡å‹, MDX23Cå’ŒHtDemucsç±»æ¨¡å‹å¯ä»¥ç›´æ¥ä½¿ç”¨å‰é¢çš„MSSTéŸ³é¢‘åˆ†ç¦»ã€‚<br>ä½¿ç”¨UVRæ¨¡å‹è¿›è¡ŒéŸ³é¢‘åˆ†ç¦»æ—¶, è‹¥æœ‰å¯ç”¨çš„GPU, è½¯ä»¶å°†è‡ªåŠ¨é€‰æ‹©, å¦åˆ™å°†ä½¿ç”¨CPUè¿›è¡Œåˆ†ç¦»ã€‚<br>UVRåˆ†ç¦»ä½¿ç”¨é¡¹ç›®: [https://github.com/nomadkaraoke/python-audio-separator](https://github.com/nomadkaraoke/python-audio-separator) å¹¶è¿›è¡Œäº†ä¼˜åŒ–ã€‚"))
            vr_select_model = gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹"), choices=load_vr_model(), value=webui_config['inference']['vr_select_model'] if webui_config['inference']['vr_select_model'] else None,interactive=True)
            with gr.Row():
                vr_window_size = gr.Dropdown(label=i18n("Window Size: çª—å£å¤§å°, ç”¨äºå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡"), choices=["320", "512", "1024"], value="512", interactive=True)
                vr_aggression = gr.Number(label=i18n("Aggression: ä¸»å¹²æå–å¼ºåº¦, èŒƒå›´-100-100, äººå£°è¯·é€‰5"), minimum=-100, maximum=100, value=5, interactive=True)
                vr_output_format = gr.Dropdown(label=i18n("è¾“å‡ºæ ¼å¼"), choices=["wav", "flac", "mp3"], value="wav", interactive=True)
            with gr.Row():
                vr_primary_stem_label, vr_secondary_stem_label = init_selected_vr_model()
                vr_use_cpu = gr.Checkbox(label=i18n("ä½¿ç”¨CPU"), value=False, interactive=True)
                vr_primary_stem_only = gr.Checkbox(label=vr_primary_stem_label, value=False, interactive=True)
                vr_secondary_stem_only = gr.Checkbox(label=vr_secondary_stem_label, value=False, interactive=True)
            vr_multiple_audio_input = gr.Textbox(label=i18n("è¾“å…¥ç›®å½•"),value="input/",interactive=True)
            vr_store_dir = gr.Textbox(label=i18n("è¾“å‡ºç›®å½•"), value="results/", interactive=True)
            with gr.Accordion(i18n("ä»¥ä¸‹æ˜¯ä¸€äº›é«˜çº§è®¾ç½®, ä¸€èˆ¬ä¿æŒé»˜è®¤å³å¯"), open=False):
                with gr.Row():
                    vr_batch_size = gr.Number(label=i18n("Batch Size: ä¸€æ¬¡è¦å¤„ç†çš„æ‰¹æ¬¡æ•°, è¶Šå¤§å ç”¨è¶Šå¤šRAM, å¤„ç†é€Ÿåº¦åŠ å¿«"), minimum=1, value=2, interactive=True)
                    vr_normalization = gr.Number(label=i18n("Normalization: æœ€å¤§å³°å€¼æŒ¯å¹…, ç”¨äºå½’ä¸€åŒ–è¾“å…¥å’Œè¾“å‡ºéŸ³é¢‘ã€‚å–å€¼ä¸º0-1"), minimum=0.0, maximum=1.0, step=0.01, value=1, interactive=True)
                    vr_post_process_threshold = gr.Number(label=i18n("Post Process Threshold: åå¤„ç†ç‰¹å¾é˜ˆå€¼, å–å€¼ä¸º0.1-0.3"), minimum=0.1, maximum=0.3, step=0.01, value=0.2, interactive=True)
                with gr.Row():
                    vr_invert_spect = gr.Checkbox(label=i18n("Invert Spectrogram: äºŒçº§æ­¥éª¤å°†ä½¿ç”¨é¢‘è°±å›¾è€Œéæ³¢å½¢è¿›è¡Œåè½¬, å¯èƒ½ä¼šæé«˜è´¨é‡, ä½†é€Ÿåº¦ç¨æ…¢"), value=False, interactive=True)
                    vr_enable_tta = gr.Checkbox(label=i18n("Enable TTA: å¯ç”¨â€œæµ‹è¯•æ—¶é—´å¢å¼ºâ€, å¯èƒ½ä¼šæé«˜è´¨é‡, ä½†é€Ÿåº¦ç¨æ…¢"), value=False, interactive=True)
                    vr_high_end_process = gr.Checkbox(label=i18n("High End Process: å°†è¾“å‡ºéŸ³é¢‘ç¼ºå¤±çš„é¢‘ç‡èŒƒå›´é•œåƒè¾“å‡º"), value=False, interactive=True)
                    vr_enable_post_process = gr.Checkbox(label=i18n("Enable Post Process: è¯†åˆ«äººå£°è¾“å‡ºä¸­æ®‹ç•™çš„äººå·¥ç—•è¿¹, å¯æ”¹å–„æŸäº›æ­Œæ›²çš„åˆ†ç¦»æ•ˆæœ"), value=False, interactive=True)
                vr_debug_mode = gr.Checkbox(label=i18n("Debug Mode: å¯ç”¨è°ƒè¯•æ¨¡å¼, å‘å¼€å‘äººå‘˜åé¦ˆæ—¶, è¯·å¼€å¯æ­¤æ¨¡å¼"), value=False, interactive=True)
            vr_start_multi_inference = gr.Button(i18n("æ‰¹é‡éŸ³é¢‘åˆ†ç¦»"), variant="primary")
            with gr.Row():
                vr_output_message = gr.Textbox(label="Output Message", scale=4)
                stop_thread = gr.Button(i18n("å¼ºåˆ¶åœæ­¢"), scale=1)

            vr_select_model.change(fn=load_vr_model_stem,inputs=vr_select_model,outputs=[vr_primary_stem_only, vr_secondary_stem_only])
            vr_start_multi_inference.click(fn=vr_inference_multi,inputs=[vr_select_model, vr_window_size, vr_aggression, vr_output_format, vr_use_cpu, vr_primary_stem_only, vr_secondary_stem_only, vr_multiple_audio_input, vr_store_dir, vr_batch_size, vr_normalization, vr_post_process_threshold, vr_invert_spect, vr_enable_tta, vr_high_end_process, vr_enable_post_process, vr_debug_mode],outputs=vr_output_message)
            stop_thread.click(fn=stop_all_thread)

        with gr.TabItem(label=i18n("é¢„è®¾æµç¨‹")):
            gr.Markdown(value=i18n("é¢„è®¾æµç¨‹å…è®¸æŒ‰ç…§é¢„è®¾çš„é¡ºåºè¿è¡Œå¤šä¸ªæ¨¡å‹ã€‚æ¯ä¸€ä¸ªæ¨¡å‹çš„è¾“å‡ºå°†ä½œä¸ºä¸‹ä¸€ä¸ªæ¨¡å‹çš„è¾“å…¥ã€‚"))
            with gr.Tabs():
                with gr.TabItem(label=i18n("ä½¿ç”¨é¢„è®¾")):
                    preset_dropdown = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"),choices=list(presets.keys()),value=None,interactive=True)
                    force_cpu = gr.Checkbox(label=i18n("ä½¿ç”¨CPU (æ³¨æ„: ä½¿ç”¨CPUä¼šå¯¼è‡´é€Ÿåº¦éå¸¸æ…¢) "), value=False, interactive=True)
                    input_folder_flow = gr.Textbox(label=i18n("è¾“å…¥ç›®å½•"), value="input/", interactive=True)
                    store_dir_flow = gr.Textbox(label=i18n("è¾“å‡ºç›®å½•"), value="results/", interactive=True)
                    inference_flow = gr.Button(i18n("æ‰¹é‡éŸ³é¢‘åˆ†ç¦»"), variant="primary")
                    with gr.Row():
                        output_message_flow = gr.Textbox(label="Output Message", scale=4)
                        stop_thread = gr.Button(i18n("å¼ºåˆ¶åœæ­¢"), scale=1)
                with gr.TabItem(label=i18n("åˆ¶ä½œé¢„è®¾")):
                    gr.Markdown(i18n("æ³¨æ„: MSSTæ¨¡å‹ä»…æ”¯æŒè¾“å‡ºä¸»è¦éŸ³è½¨, UVRæ¨¡å‹æ”¯æŒè‡ªå®šä¹‰ä¸»è¦éŸ³è½¨è¾“å‡ºã€‚<br>åŒæ—¶è¾“å‡ºæ¬¡çº§éŸ³è½¨: é€‰æ‹©Trueå°†åŒæ—¶è¾“å‡ºè¯¥æ¬¡åˆ†ç¦»å¾—åˆ°çš„æ¬¡çº§éŸ³è½¨, **æ­¤éŸ³è½¨å°†ç›´æ¥ä¿å­˜è‡³**è¾“å‡ºç›®å½•ä¸‹çš„secondary_outputæ–‡ä»¶å¤¹, **ä¸ä¼šç»è¿‡åç»­æµç¨‹å¤„ç†**<br>"))
                    preset_name_input = gr.Textbox(label=i18n("é¢„è®¾åç§°"), placeholder=i18n("è¯·è¾“å…¥é¢„è®¾åç§°"), interactive=True)
                    with gr.Row():
                        model_type = gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹ç±»å‹"), choices=["MSST_Models", "UVR_VR_Models"], interactive=True)
                        model_name = gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹"), choices=[i18n("è¯·å…ˆé€‰æ‹©æ¨¡å‹ç±»å‹")], interactive=True, scale=2)
                        stem = gr.Dropdown(label=i18n("è¾“å‡ºéŸ³è½¨"), choices=[i18n("è¯·å…ˆé€‰æ‹©æ¨¡å‹")], interactive=True)
                        secondary_output = gr.Dropdown(label=i18n("åŒæ—¶è¾“å‡ºæ¬¡çº§éŸ³è½¨"), choices=["True", "False"], value="False", interactive=True)
                    add_to_flow = gr.Button(i18n("æ·»åŠ è‡³æµç¨‹"))
                    gr.Markdown(i18n("é¢„è®¾æµç¨‹"))
                    preset_flow = gr.Dataframe(pd.DataFrame({"model_type": [""], "model_name": [""], "stem": [""], "secondary_output": [""]}), interactive=False, label=None)
                    reset_flow = gr.Button(i18n("é‡æ–°è¾“å…¥"))
                    save_flow = gr.Button(i18n("ä¿å­˜ä¸Šè¿°é¢„è®¾æµç¨‹"), variant="primary")
                    output_message_make = gr.Textbox(label="Output Message")
                with gr.TabItem(label=i18n("ç®¡ç†é¢„è®¾")):
                    gr.Markdown(i18n("æ­¤é¡µé¢æä¾›æŸ¥çœ‹é¢„è®¾, åˆ é™¤é¢„è®¾, å¤‡ä»½é¢„è®¾, æ¢å¤é¢„è®¾ç­‰åŠŸèƒ½"))
                    preset_name_delete = gr.Dropdown(label=i18n("è¯·é€‰æ‹©é¢„è®¾"), choices=load_presets_list(), interactive=True)
                    gr.Markdown(i18n("`model_type`: æ¨¡å‹ç±»å‹ï¼›`model_name`: æ¨¡å‹åç§°ï¼›`stem`: ä¸»è¦è¾“å‡ºéŸ³è½¨ï¼›<br>`secondary_output`: åŒæ—¶è¾“å‡ºæ¬¡çº§éŸ³è½¨ã€‚é€‰æ‹©Trueå°†åŒæ—¶è¾“å‡ºè¯¥æ¬¡åˆ†ç¦»å¾—åˆ°çš„æ¬¡çº§éŸ³è½¨, **æ­¤éŸ³è½¨å°†ç›´æ¥ä¿å­˜è‡³**è¾“å‡ºç›®å½•ä¸‹çš„secondary_outputæ–‡ä»¶å¤¹, **ä¸ä¼šç»è¿‡åç»­æµç¨‹å¤„ç†**"))
                    preset_flow_delete = gr.Dataframe(pd.DataFrame({"model_type": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")], "model_name": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")], "stem": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")], "secondary_output": [i18n("è¯·å…ˆé€‰æ‹©é¢„è®¾")]}), interactive=False, label=None)
                    delete_button = gr.Button(i18n("åˆ é™¤æ‰€é€‰é¢„è®¾"), scale=1)
                    gr.Markdown(i18n("æ¯æ¬¡åˆ é™¤é¢„è®¾å‰, å°†è‡ªåŠ¨å¤‡ä»½é¢„è®¾ä»¥å…è¯¯æ“ä½œã€‚<br>ä½ ä¹Ÿå¯ä»¥ç‚¹å‡»â€œå¤‡ä»½é¢„è®¾æµç¨‹â€æŒ‰é’®è¿›è¡Œæ‰‹åŠ¨å¤‡ä»½, ä¹Ÿå¯ä»¥ä»å¤‡ä»½æ–‡ä»¶å¤¹ä¸­æ¢å¤é¢„è®¾æµç¨‹ã€‚"))
                    backup_preset = gr.Button(i18n("å¤‡ä»½é¢„è®¾æµç¨‹"))
                    with gr.Row():
                        select_preset_backup = gr.Dropdown(label=i18n("é€‰æ‹©éœ€è¦æ¢å¤çš„é¢„è®¾æµç¨‹å¤‡ä»½"),choices=preset_backup_list(),interactive=True,scale=4)
                        restore_preset = gr.Button(i18n("æ¢å¤"), scale=1)
                    output_message_manage = gr.Textbox(label="Output Message")
                

            inference_flow.click(fn=run_inference_flow,inputs=[input_folder_flow, store_dir_flow, preset_dropdown, force_cpu],outputs=output_message_flow)
            model_type.change(update_model_name, inputs=model_type, outputs=model_name)
            model_name.change(update_model_stem, inputs=[model_type, model_name], outputs=stem)
            add_to_flow.click(add_to_flow_func, [model_type, model_name, stem, secondary_output, preset_flow], preset_flow)
            save_flow.click(save_flow_func, [preset_name_input, preset_flow], [output_message_make, preset_name_delete, preset_dropdown])
            reset_flow.click(reset_flow_func, [], preset_flow)
            delete_button.click(delete_func, [preset_name_delete], [output_message_manage, preset_name_delete, preset_dropdown, preset_flow_delete, select_preset_backup])
            preset_name_delete.change(load_preset, inputs=preset_name_delete, outputs=preset_flow_delete)
            stop_thread.click(fn=stop_all_thread)
            restore_preset.click(fn=restore_preset_func,inputs=[select_preset_backup],outputs=[output_message_manage, preset_dropdown, preset_name_delete, preset_flow_delete])
            backup_preset.click(fn=backup_preset_func,outputs=[output_message_manage, select_preset_backup])

        with gr.TabItem(label=i18n("å°å·¥å…·")):
            with gr.Tabs():
                with gr.TabItem(label=i18n("éŸ³é¢‘æ ¼å¼è½¬æ¢")):
                    gr.Markdown(value=i18n("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªéŸ³é¢‘æ–‡ä»¶å¹¶å°†å…¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ã€‚<br>æ”¯æŒçš„æ ¼å¼åŒ…æ‹¬ .mp3, .flac, .wav, .ogg, .m4a, .wma, .aac...ç­‰ç­‰ã€‚<br>**ä¸æ”¯æŒ**ç½‘æ˜“äº‘éŸ³ä¹/QQéŸ³ä¹ç­‰åŠ å¯†æ ¼å¼, å¦‚.ncm, .qmcç­‰ã€‚"))
                    with gr.Row():
                        inputs = gr.Textbox(label=i18n("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"), value="input/", interactive=True, scale=4)
                        ffmpeg_output_format = gr.Dropdown(label=i18n("é€‰æ‹©æˆ–è¾“å…¥éŸ³é¢‘è¾“å‡ºæ ¼å¼"), choices=["wav", "flac", "mp3", "ogg", "m4a", "wma", "aac"], value="wav", allow_custom_value=True, interactive=True, scale=2)
                    ffmpeg_output_folder = gr.Textbox(label=i18n("é€‰æ‹©éŸ³é¢‘è¾“å‡ºç›®å½•"), value="results/ffmpeg_output/", interactive=True)
                    convert_audio_button = gr.Button(i18n("è½¬æ¢éŸ³é¢‘"), variant="primary")
                    output_message_ffmpeg = gr.Textbox(label="Output Message")
                with gr.TabItem(label=i18n("åˆå¹¶éŸ³é¢‘")):
                    gr.Markdown(value=i18n("ç‚¹å‡»åˆå¹¶éŸ³é¢‘æŒ‰é’®å, å°†è‡ªåŠ¨æŠŠè¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶åˆå¹¶ä¸ºä¸€æ•´ä¸ªéŸ³é¢‘æ–‡ä»¶<br>ç›®å‰æ”¯æŒçš„æ ¼å¼åŒ…æ‹¬ .mp3, .flac, .wav, .ogg è¿™å››ç§<br>åˆå¹¶åçš„éŸ³é¢‘ä¼šä¿å­˜è‡³è¾“å‡ºç›®å½•ä¸­, æ–‡ä»¶åä¸ºmerged_audio.wav"))
                    merge_audio_input = gr.Textbox(label=i18n("è¾“å…¥ç›®å½•"),value="input/",interactive=True,scale=3)
                    merge_audio_output = gr.Textbox(label=i18n("è¾“å‡ºç›®å½•"),value="results/merge",interactive=True,scale=3)
                    merge_audio_button = gr.Button(i18n("åˆå¹¶éŸ³é¢‘"), variant="primary")
                    output_message_merge = gr.Textbox(label="Output Message")
                with gr.TabItem(label=i18n("è®¡ç®—SDR")):
                    gr.Markdown(value=i18n("ä¸Šä¼ ä¸¤ä¸ª**wavéŸ³é¢‘æ–‡ä»¶**å¹¶è®¡ç®—å®ƒä»¬çš„[SDR](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021#evaluation-metric)ã€‚<br>SDRæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨¡å‹è´¨é‡çš„æ•°å€¼ã€‚æ•°å€¼è¶Šå¤§, æ¨¡å‹ç®—æ³•ç»“æœè¶Šå¥½ã€‚"))
                    with gr.Row():
                        true_audio = gr.Textbox(label=i18n("åŸå§‹éŸ³é¢‘"), value="input/no_audio.wav", interactive=True)
                        estimated_audio = gr.Textbox(label=i18n("åˆ†ç¦»åçš„éŸ³é¢‘"), value="input/no_audio.wav", interactive=True)
                    compute_sdr_button = gr.Button(i18n("è®¡ç®—SDR"), variant="primary")
                    output_message_sdr = gr.Textbox(label="Output Message")
                with gr.TabItem(label = i18n("Ensembleæ¨¡å¼")):
                    gr.Markdown(value = i18n("å¯ç”¨äºé›†æˆä¸åŒç®—æ³•çš„ç»“æœã€‚å…·ä½“çš„æ–‡æ¡£ä½äº/docs/ensemble.md"))
                    files = gr.Textbox(label=i18n("ä¸Šä¼ å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"), value = "input/", interactive=True)
                    with gr.Row():
                        ensemble_type = gr.Dropdown(choices = ["avg_wave", "median_wave", "min_wave", "max_wave", "avg_fft", "median_fft", "min_fft", "max_fft"],label = i18n("é›†æˆæ¨¡å¼"), value="avg_wave", interactive=True)
                        weights = gr.Textbox(label = i18n("æƒé‡(ä»¥ç©ºæ ¼åˆ†éš”, æ•°é‡è¦ä¸ä¸Šä¼ çš„éŸ³é¢‘ä¸€è‡´)"), value="1 1")
                    ensembl_output_path = gr.Textbox(label = i18n("è¾“å‡ºç›®å½•"), value="results/ensemble/", interactive=True)
                    ensemble_button = gr.Button(i18n("è¿è¡Œ"), variant="primary")
                    output_message_ensemble = gr.Textbox(label = "Output Message")
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown(i18n("### é›†æˆæ¨¡å¼"))
                            gr.Markdown(i18n("1. `avg_wave`: åœ¨1Då˜ä½“ä¸Šè¿›è¡Œé›†æˆ, ç‹¬ç«‹åœ°æ‰¾åˆ°æ³¢å½¢çš„æ¯ä¸ªæ ·æœ¬çš„å¹³å‡å€¼<br>2. `median_wave`: åœ¨1Då˜ä½“ä¸Šè¿›è¡Œé›†æˆ, ç‹¬ç«‹åœ°æ‰¾åˆ°æ³¢å½¢çš„æ¯ä¸ªæ ·æœ¬çš„ä¸­ä½æ•°<br>3. `min_wave`: åœ¨1Då˜ä½“ä¸Šè¿›è¡Œé›†æˆ, ç‹¬ç«‹åœ°æ‰¾åˆ°æ³¢å½¢çš„æ¯ä¸ªæ ·æœ¬çš„æœ€å°ç»å¯¹å€¼<br>4. `max_wave`: åœ¨1Då˜ä½“ä¸Šè¿›è¡Œé›†æˆ, ç‹¬ç«‹åœ°æ‰¾åˆ°æ³¢å½¢çš„æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§ç»å¯¹å€¼<br>5. `avg_fft`: åœ¨é¢‘è°±å›¾ (çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ (STFT) 2Då˜ä½“) ä¸Šè¿›è¡Œé›†æˆ, ç‹¬ç«‹åœ°æ‰¾åˆ°é¢‘è°±å›¾çš„æ¯ä¸ªåƒç´ çš„å¹³å‡å€¼ã€‚å¹³å‡åä½¿ç”¨é€†STFTå¾—åˆ°åŸå§‹çš„1Dæ³¢å½¢<br>6. `median_fft`: ä¸avg_fftç›¸åŒ, ä½†ä½¿ç”¨ä¸­ä½æ•°ä»£æ›¿å¹³å‡å€¼ (ä»…åœ¨é›†æˆ3ä¸ªæˆ–æ›´å¤šæ¥æºæ—¶æœ‰ç”¨) <br>7. `min_fft`: ä¸avg_fftç›¸åŒ, ä½†ä½¿ç”¨æœ€å°å‡½æ•°ä»£æ›¿å¹³å‡å€¼ (å‡å°‘æ¿€è¿›ç¨‹åº¦) <br>8. `max_fft`: ä¸avg_fftç›¸åŒ, ä½†ä½¿ç”¨æœ€å¤§å‡½æ•°ä»£æ›¿å¹³å‡å€¼ (å¢åŠ æ¿€è¿›ç¨‹åº¦) "))
                        with gr.Column():
                            gr.Markdown(i18n("### æ³¨æ„äº‹é¡¹"))
                            gr.Markdown(i18n("1. min_fftå¯ç”¨äºè¿›è¡Œæ›´ä¿å®ˆçš„åˆæˆ, å®ƒå°†å‡å°‘æ›´æ¿€è¿›æ¨¡å‹çš„å½±å“ã€‚<br>2. æœ€å¥½åˆæˆç­‰è´¨é‡çš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹, å®ƒå°†å¸¦æ¥å¢ç›Šã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªæ¨¡å‹è´¨é‡ä¸å¥½, å®ƒå°†é™ä½æ•´ä½“è´¨é‡ã€‚<br>3. åœ¨åŸä»“åº“ä½œè€…çš„å®éªŒä¸­, ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”, avg_waveåœ¨SDRåˆ†æ•°ä¸Šæ€»æ˜¯æ›´å¥½æˆ–ç›¸ç­‰ã€‚<br>4. ä¸Šä¼ çš„æ–‡ä»¶å**ä¸èƒ½åŒ…å«ç©ºæ ¼**, æœ€ç»ˆä¼šåœ¨è¾“å‡ºç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ª`ensemble_<é›†æˆæ¨¡å¼>.wav`ã€‚"))
                with gr.TabItem(label=i18n("æ­Œå£°è½¬MIDI")):
                    gr.Markdown(value=i18n("æ­Œå£°è½¬MIDIåŠŸèƒ½ä½¿ç”¨å¼€æºé¡¹ç›®[SOME](https://github.com/openvpi/SOME/), å¯ä»¥å°†åˆ†ç¦»å¾—åˆ°çš„**å¹²å‡€çš„æ­Œå£°**è½¬æ¢æˆ.midæ–‡ä»¶ã€‚<br>ã€å¿…é¡»ã€‘è‹¥æƒ³è¦ä½¿ç”¨æ­¤åŠŸèƒ½, è¯·å…ˆä¸‹è½½æƒé‡æ–‡ä»¶[model_steps_64000_simplified.ckpt](https://hf-mirror.com/Sucial/SOME_Models/resolve/main/model_steps_64000_simplified.ckpt)å¹¶å°†å…¶æ”¾ç½®åœ¨ç¨‹åºç›®å½•ä¸‹çš„`tools/SOME_weights`æ–‡ä»¶å¤¹å†…ã€‚æ–‡ä»¶å‘½åä¸å¯éšæ„æ›´æ”¹! <br>ã€é‡è¦ã€‘åªèƒ½ä¸Šä¼ wavæ ¼å¼çš„éŸ³é¢‘! "))
                    some_input_audio = gr.Textbox(label=i18n("ä¸Šä¼ wavæ ¼å¼éŸ³é¢‘"), value="input/no_audio.wav", interactive=True)
                    audio_bpm = gr.Number(label=i18n("è¾“å…¥éŸ³é¢‘BPM"), value=120, interactive=True)
                    some_output_folder = gr.Textbox(label=i18n("è¾“å‡ºç›®å½•"),value="results/some/", interactive=True)
                    some_button = gr.Button(i18n("å¼€å§‹è½¬æ¢"), variant="primary")
                    output_message_some = gr.Textbox(label="Output Message")
                    gr.Markdown(i18n("### æ³¨æ„äº‹é¡¹"))
                    gr.Markdown(i18n("1. éŸ³é¢‘BPM (æ¯åˆ†é’ŸèŠ‚æ‹æ•°) å¯ä»¥é€šè¿‡MixMeister BPM Analyzerç­‰è½¯ä»¶æµ‹é‡è·å–ã€‚<br>2. ä¸ºä¿è¯MIDIæå–è´¨é‡, éŸ³é¢‘æ–‡ä»¶è¯·é‡‡ç”¨å¹²å‡€æ¸…æ™°æ— æ··å“åº•å™ªäººå£°ã€‚<br>3. è¾“å‡ºMIDIä¸å¸¦æ­Œè¯ä¿¡æ¯, éœ€è¦ç”¨æˆ·è‡ªè¡Œæ·»åŠ æ­Œè¯ã€‚<br>4. å®é™…ä½¿ç”¨ä½“éªŒä¸­éƒ¨åˆ†éŸ³ç¬¦ä¼šå‡ºç°æ–­å¼€çš„ç°è±¡, éœ€è‡ªè¡Œä¿®æ­£ã€‚SOMEçš„æ¨¡å‹ä¸»è¦é¢å‘DiffSingerå”±æ³•æ¨¡å‹è‡ªåŠ¨æ ‡æ³¨, æ¯”æ­£å¸¸ç”¨æˆ·åœ¨åˆ›ä½œä¸­éœ€è¦çš„MIDIæ›´åŠ ç²¾ç»†, å› è€Œå¯èƒ½å¯¼è‡´æ¨¡å‹å€¾å‘äºå¯¹éŸ³ç¬¦è¿›è¡Œåˆ‡åˆ†ã€‚<br>5. æå–çš„MIDIæ²¡æœ‰é‡åŒ–/æ²¡æœ‰å¯¹é½èŠ‚æ‹/ä¸é€‚é…BPM, éœ€è‡ªè¡Œåˆ°å„ç¼–è¾‘å™¨ä¸­æ‰‹åŠ¨è°ƒæ•´ã€‚"))

            convert_audio_button.click(fn=convert_audio, inputs=[inputs, ffmpeg_output_format, ffmpeg_output_folder], outputs=output_message_ffmpeg)
            merge_audio_button.click(merge_audios, [merge_audio_input, merge_audio_output], outputs=output_message_merge)
            compute_sdr_button.click(process_audio, [true_audio, estimated_audio], outputs=output_message_sdr)
            ensemble_button.click(fn = ensemble, inputs = [files, ensemble_type, weights, ensembl_output_path],outputs = output_message_ensemble)
            some_button.click(fn=some_inference,inputs=[some_input_audio, audio_bpm, some_output_folder],outputs=output_message_some)

        with gr.TabItem(label=i18n("MSSTè®­ç»ƒ")):
            gr.Markdown(value=i18n("æ­¤é¡µé¢æä¾›æ•°æ®é›†åˆ¶ä½œæ•™ç¨‹, è®­ç»ƒå‚æ•°é€‰æ‹©, ä»¥åŠä¸€é”®è®­ç»ƒã€‚æœ‰å…³é…ç½®æ–‡ä»¶çš„ä¿®æ”¹å’Œæ•°æ®é›†æ–‡ä»¶å¤¹çš„è¯¦ç»†è¯´æ˜è¯·å‚è€ƒMSSTåŸé¡¹ç›®: [https://github.com/ZFTurbo/Music-Source-Separation-Training](https://github.com/ZFTurbo/Music-Source-Separation-Training)<br>åœ¨å¼€å§‹ä¸‹æ–¹çš„æ¨¡å‹è®­ç»ƒä¹‹å‰, è¯·å…ˆè¿›è¡Œè®­ç»ƒæ•°æ®çš„åˆ¶ä½œã€‚"))
            with gr.Tabs():
                with gr.TabItem(label=i18n("è®­ç»ƒ")):
                    with gr.Row():
                        train_model_type = gr.Dropdown(label=i18n("é€‰æ‹©è®­ç»ƒæ¨¡å‹ç±»å‹"),choices=MODEL_TYPE,value=webui_config['training']['model_type'] if webui_config['training']['model_type'] else None,interactive=True,scale=1)
                        train_config_path = gr.Textbox(label=i18n("é…ç½®æ–‡ä»¶è·¯å¾„"),value=webui_config['training']['config_path'] if webui_config['training']['config_path'] else i18n("è¯·è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„æˆ–é€‰æ‹©é…ç½®æ–‡ä»¶"),interactive=True,scale=3)
                    with gr.Row():
                        train_dataset_type = gr.Dropdown(label=i18n("æ•°æ®é›†ç±»å‹"),choices=[1, 2, 3, 4],value=webui_config['training']['dataset_type'] if webui_config['training']['dataset_type'] else None,interactive=True,scale=1)
                        train_dataset_path = gr.Textbox(label=i18n("æ•°æ®é›†è·¯å¾„"),value=webui_config['training']['dataset_path'] if webui_config['training']['dataset_path'] else i18n("è¯·è¾“å…¥æˆ–é€‰æ‹©æ•°æ®é›†æ–‡ä»¶å¤¹"),interactive=True,scale=3)
                    gr.Markdown(value=i18n("è¯´æ˜: æ•°æ®é›†ç±»å‹å³è®­ç»ƒé›†åˆ¶ä½œStep 1ä¸­ä½ é€‰æ‹©çš„ç±»å‹, 1: Type1; 2: Type2; 3: Type3; 4: Type4, å¿…é¡»ä¸ä½ çš„æ•°æ®é›†ç±»å‹ç›¸åŒ¹é…ã€‚"))
                    train_valid_path = gr.Textbox(label=i18n("éªŒè¯é›†è·¯å¾„"),value=webui_config['training']['valid_path'] if webui_config['training']['valid_path'] else i18n("è¯·è¾“å…¥æˆ–é€‰æ‹©éªŒè¯é›†æ–‡ä»¶å¤¹"),interactive=True,scale=4)
                    with gr.Row():
                        train_num_workers = gr.Number(label=i18n("num_workers: æ•°æ®é›†è¯»å–çº¿ç¨‹æ•°, 0ä¸ºè‡ªåŠ¨"),value=webui_config['training']['num_workers'] if webui_config['training']['num_workers'] else 0,interactive=True,minimum=0,maximum=cpu_count(),step=1)
                        train_device_ids = gr.Textbox(label=i18n("device_ids: é€‰æ‹©æ˜¾å¡, å¤šå¡ç”¨æˆ·è¯·ä½¿ç”¨ç©ºæ ¼åˆ†éš”"),value=webui_config['training']['device_ids'] if webui_config['training']['device_ids'] else "0",interactive=True)
                        train_seed = gr.Number(label=i18n("éšæœºæ•°ç§å­, 0ä¸ºéšæœº"), value="0")
                    with gr.Row():
                        train_pin_memory = gr.Checkbox(label=i18n("æ˜¯å¦å°†åŠ è½½çš„æ•°æ®æ”¾ç½®åœ¨å›ºå®šå†…å­˜ä¸­, é»˜è®¤ä¸ºå¦"), value=webui_config['training']['pin_memory'], interactive=True)
                        train_use_multistft_loss = gr.Checkbox(label=i18n("æ˜¯å¦ä½¿ç”¨MultiSTFT Loss, é»˜è®¤ä¸ºå¦"), value=webui_config['training']['use_multistft_loss'], interactive=True)
                        train_use_mse_loss = gr.Checkbox(label=i18n("æ˜¯å¦ä½¿ç”¨MSE loss, é»˜è®¤ä¸ºå¦"), value=webui_config['training']['use_mse_loss'], interactive=True)
                        train_use_l1_loss = gr.Checkbox(label=i18n("æ˜¯å¦ä½¿ç”¨L1 loss, é»˜è®¤ä¸ºå¦"), value=webui_config['training']['use_l1_loss'], interactive=True)
                        train_accelerate = gr.Checkbox(label=i18n("(å®éªŒä¸­) æ˜¯å¦ä½¿ç”¨åŠ é€Ÿè®­ç»ƒ, å¯¹äºå¤šæ˜¾å¡ç”¨æˆ·ä¼šåŠ å¿«è®­ç»ƒ"), value=False, interactive=True)
                    with gr.Row():
                        train_results_path = gr.Textbox(label=i18n("æ¨¡å‹ä¿å­˜è·¯å¾„"),value=webui_config['training']['results_path'] if webui_config['training']['results_path'] else i18n("è¯·è¾“å…¥æˆ–é€‰æ‹©æ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹"),interactive=True,scale=3)
                    with gr.Row():
                        train_start_check_point = gr.Dropdown(label=i18n("åˆå§‹æ¨¡å‹: ç»§ç»­è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹è®­ç»ƒæ—¶, è¯·é€‰æ‹©åˆå§‹æ¨¡å‹, å¦åˆ™å°†ä»å¤´å¼€å§‹è®­ç»ƒ! "), choices=["None"], value="None", interactive=True, scale=4)
                        reflesh_start_check_point = gr.Button(i18n("åˆ·æ–°åˆå§‹æ¨¡å‹åˆ—è¡¨"), scale=1)
                    save_train_config = gr.Button(i18n("ä¿å­˜ä¸Šè¿°è®­ç»ƒé…ç½®"))
                    start_train_button = gr.Button(i18n("å¼€å§‹è®­ç»ƒ"), variant="primary")
                    gr.Markdown(value=i18n("ç‚¹å‡»å¼€å§‹è®­ç»ƒå, è¯·åˆ°ç»ˆç«¯æŸ¥çœ‹è®­ç»ƒè¿›åº¦æˆ–æŠ¥é”™, ä¸‹æ–¹ä¸ä¼šè¾“å‡ºæŠ¥é”™ä¿¡æ¯, æƒ³è¦åœæ­¢è®­ç»ƒå¯ä»¥ç›´æ¥å…³é—­ç»ˆç«¯ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­, ä½ ä¹Ÿå¯ä»¥å…³é—­ç½‘é¡µ, ä»…**ä¿ç•™ç»ˆç«¯**ã€‚"))
                    with gr.Row():
                        output_message_train = gr.Textbox(label="Output Message", scale=4)
                        stop_thread = gr.Button(i18n("å¼ºåˆ¶åœæ­¢"), scale=1)

                    save_train_config.click(fn=save_training_config,inputs=[train_model_type, train_config_path, train_dataset_type, train_dataset_path, train_valid_path, train_num_workers,train_device_ids, train_seed, train_pin_memory, train_use_multistft_loss, train_use_mse_loss, train_use_l1_loss, train_results_path, train_accelerate],outputs=output_message_train)
                    start_train_button.click(fn=start_training,inputs=[train_model_type, train_config_path, train_dataset_type, train_dataset_path, train_valid_path, train_num_workers, train_device_ids,train_seed, train_pin_memory, train_use_multistft_loss, train_use_mse_loss, train_use_l1_loss, train_results_path, train_start_check_point, train_accelerate],outputs=output_message_train)
                    reflesh_start_check_point.click(fn=update_train_start_check_point,inputs=train_results_path,outputs=train_start_check_point)
                    stop_thread.click(fn=stop_all_thread)

                with gr.TabItem(label=i18n("éªŒè¯")):
                    gr.Markdown(value=i18n("æ­¤é¡µé¢ç”¨äºæ‰‹åŠ¨éªŒè¯æ¨¡å‹æ•ˆæœ, æµ‹è¯•éªŒè¯é›†, è¾“å‡ºSDRæµ‹è¯•ä¿¡æ¯ã€‚è¾“å‡ºçš„ä¿¡æ¯ä¼šå­˜æ”¾åœ¨è¾“å‡ºæ–‡ä»¶å¤¹çš„results.txtä¸­ã€‚<br>ä¸‹æ–¹å‚æ•°å°†è‡ªåŠ¨åŠ è½½è®­ç»ƒé¡µé¢çš„å‚æ•°, åœ¨è®­ç»ƒé¡µé¢ç‚¹å‡»ä¿å­˜è®­ç»ƒå‚æ•°å, é‡å¯WebUIå³å¯è‡ªåŠ¨åŠ è½½ã€‚å½“ç„¶ä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨è¾“å…¥å‚æ•°ã€‚<br>"))
                    with gr.Row():
                        valid_model_type = gr.Dropdown(label=i18n("é€‰æ‹©æ¨¡å‹ç±»å‹"),choices=MODEL_TYPE,value=webui_config['training']['model_type'] if webui_config['training']['model_type'] else None,interactive=True,scale=1)
                        valid_config_path = gr.Textbox(label=i18n("é…ç½®æ–‡ä»¶è·¯å¾„"),value=webui_config['training']['config_path'] if webui_config['training']['config_path'] else i18n("è¯·è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„æˆ–é€‰æ‹©é…ç½®æ–‡ä»¶"),interactive=True,scale=3)
                    valid_model_path = gr.Textbox(label=i18n("æ¨¡å‹è·¯å¾„"),value=i18n("è¯·è¾“å…¥æˆ–é€‰æ‹©æ¨¡å‹æ–‡ä»¶"),interactive=True,scale=4)
                    valid_path = gr.Textbox(label=i18n("éªŒè¯é›†è·¯å¾„"),value=webui_config['training']['valid_path'] if webui_config['training']['valid_path'] else i18n("è¯·è¾“å…¥æˆ–é€‰æ‹©éªŒè¯é›†æ–‡ä»¶å¤¹"),interactive=True,scale=4)
                    valid_results_path = gr.Textbox(label=i18n("è¾“å‡ºç›®å½•"),value="results/",interactive=True,scale=3)
                    with gr.Row():
                        valid_device_ids = gr.Textbox(label=i18n("é€‰æ‹©æ˜¾å¡, å¤šå¡ç”¨æˆ·è¯·ä½¿ç”¨ç©ºæ ¼åˆ†éš”GPU ID"),value=webui_config['training']['device_ids'] if webui_config['training']['device_ids'] else "0",interactive=True)
                        valid_num_workers = gr.Number(label=i18n("éªŒè¯é›†è¯»å–çº¿ç¨‹æ•°, 0ä¸ºè‡ªåŠ¨"),value=webui_config['training']['num_workers'] if webui_config['training']['num_workers'] else 0,interactive=True,minimum=0,maximum=cpu_count(),step=1)
                        valid_extension = gr.Dropdown(label=i18n("é€‰æ‹©éªŒè¯é›†éŸ³é¢‘æ ¼å¼"),choices=["wav", "flac", "mp3"],value="wav",interactive=True,allow_custom_value=True)
                    valid_pin_memory = gr.Checkbox(label=i18n("æ˜¯å¦å°†åŠ è½½çš„æ•°æ®æ”¾ç½®åœ¨å›ºå®šå†…å­˜ä¸­, é»˜è®¤ä¸ºå¦"), value=webui_config['training']['pin_memory'], interactive=True)
                    valid_button = gr.Button(i18n("å¼€å§‹éªŒè¯"), variant="primary")
                    with gr.Row():
                        valid_output_message = gr.Textbox(label="Output Message", scale=4)
                        stop_thread = gr.Button(i18n("å¼ºåˆ¶åœæ­¢"), scale=1)

                    valid_button.click(fn=validate_model,inputs=[valid_model_type, valid_config_path, valid_model_path, valid_path, valid_results_path, valid_device_ids, valid_num_workers, valid_extension, valid_pin_memory],outputs=valid_output_message)
                    stop_thread.click(fn=stop_all_thread)

        with gr.TabItem(label=i18n("è®¾ç½®")):
            with gr.Row():
                gpu_list = gr.Textbox(label=i18n("GPUä¿¡æ¯"), value=get_device(), interactive=False)
                plantform_info = gr.Textbox(label=i18n("ç³»ç»Ÿä¿¡æ¯"), value=get_platform(), interactive=False)
                set_language = gr.Dropdown(label=i18n("é€‰æ‹©è¯­è¨€"), choices=language_dict.keys(), value=get_language(), interactive=True)
            with gr.Row():
                update_message = gr.Textbox(label=i18n("æ£€æŸ¥æ›´æ–°"), value=i18n("å½“å‰ç‰ˆæœ¬: ") + PACKAGE_VERSION + i18n(", è¯·ç‚¹å‡»æ£€æŸ¥æ›´æ–°æŒ‰é’®"), interactive=False,scale=4)
                check_update = gr.Button(i18n("æ£€æŸ¥æ›´æ–°"), scale=1)
            reset_all_webui_config = gr.Button(i18n("é‡ç½®WebUIè·¯å¾„è®°å½•"), variant="primary")
            setting_output_message = gr.Textbox(label="Output Message")

            check_update.click(fn=check_webui_update, outputs=update_message)
            reset_all_webui_config.click(fn=reset_webui_config,outputs=setting_output_message)
            set_language.change(fn=change_language,inputs=[set_language],outputs=setting_output_message)

app.launch(share=True)
